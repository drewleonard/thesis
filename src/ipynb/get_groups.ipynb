{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ad(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ad_group(ad):\n",
    "    \n",
    "    if 'second' not in ad:\n",
    "        return None\n",
    "    \n",
    "    if ad == 'black matters':\n",
    "        return ad\n",
    "    \n",
    "    # Index down to get ad text\n",
    "    ad_text = ad['second']['text']['textAnnotations'][0]['description'].splitlines()\n",
    "    \n",
    "    if 'instagram' in ad_text[0].lower() or 'sponsored' in ad_text[0].lower() or 'suggested' in ad_text[0].lower():\n",
    "        return ad_text[1]\n",
    "    return ad_text[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ad_id(ad):\n",
    "    \n",
    "    # Index down\n",
    "    ad_text = ad['first']['text']['fullTextAnnotation']['text']\n",
    "    ad_id_line = ad_text.splitlines()[0]\n",
    "    \n",
    "    # Parse id\n",
    "    ad_id = re.search(r'\\d+', ad_id_line).group()\n",
    "    \n",
    "    return ad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_first_page_text(text):\n",
    "    \n",
    "    remove_tokens = ['shared', 'added', 'updated']\n",
    "    for token in remove_tokens:\n",
    "        if token in text:\n",
    "            text = text.split(token,1)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = set([\n",
    " u'black matters',\n",
    " u'afrokingdom',\n",
    " u'american.made',\n",
    " u'american.veterans',\n",
    " u'angry eagle',\n",
    " u'army of jesus',\n",
    " u'back the badge',\n",
    " u'baltimore.blackvoice',\n",
    " u'being patriotic',\n",
    " u'bernie sanders for president',\n",
    " u'black baptist church',\n",
    " u'black edification',\n",
    " u'black excellence',\n",
    " u'black guns matter',\n",
    " u'black matters',\n",
    " u'black matters us',\n",
    " u'black4black',\n",
    " u'blackluive',\n",
    " u'blacks go viral',\n",
    " u'blackstagram',\n",
    " u'blacktivist',\n",
    " u'bm',\n",
    " u'born black',\n",
    " u'born liberal',\n",
    " u'brown power',\n",
    " u'cop block us',\n",
    " u'defend the 2nd',\n",
    " u\"don't shoot\",\n",
    " u'donald trump america',\n",
    " u'fit black',\n",
    " u'gov spending',\n",
    " u'heart of texas',\n",
    " u'hell_and_back',\n",
    " u'instagnan',\n",
    " u'isagan',\n",
    " u'justice for ezell ford and donnell thompson',\n",
    " u'l for life',\n",
    " u'lgbt united',\n",
    " u'liberty_rising',\n",
    " u'make america great again donald j trump',\n",
    " u'mbm',\n",
    " u'melanie black',\n",
    " u'melanie panther',\n",
    " u'memopolis',\n",
    " u'mericanfury',\n",
    " u'muslim voice',\n",
    " u'musliminst',\n",
    " u'native americans united',\n",
    " u\"nefertiti's community\",\n",
    " u'pan-african roots move',\n",
    " u'panther melanie',\n",
    " u\"patriot's heart\",\n",
    " u'pray4police',\n",
    " u'proud blacks',\n",
    " u'rebeltexas',\n",
    " u'secured borders',\n",
    " u'sincerely_black',\n",
    " u'south united',\n",
    " u'southern.rebel.pride',\n",
    " u'stand for freedom',\n",
    " u'stop a.l.',\n",
    " u'stop refugees',\n",
    " u'the red pill',\n",
    " u'trumpsters united',\n",
    " u'united muslims of america',\n",
    " u'veterans come first',\n",
    " u'watch the police',\n",
    " u'williams&kalvin',\n",
    " u'woke blacks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set([\n",
    "u'ad creation date 03/17/16 04:21:38 am pdt',\n",
    " u'ad creation date 07/21/15 06:54:01 am pdt',\n",
    " u'ad end date 06/24/15 07:03:47 am pdt',\n",
    " u'ad impressions 1,439',\n",
    " u'ad impressions 19,055',\n",
    " u'ad impressions 306',\n",
    " u'age: 18-65+'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "candidates = set()\n",
    "rootdir = '/Users/drewnleonard/Documents/thesis/data/json/ads/'\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "            \n",
    "        path = os.path.join(subdir, file)\n",
    "        if '.json' not in path:\n",
    "            continue\n",
    "        ad = load_ad(path)\n",
    "        \n",
    "        if 'second' not in ad:\n",
    "            continue\n",
    "        \n",
    "        ad_text = ad['second']['text']['textAnnotations'][0]['description'].splitlines()\n",
    "        found_group = clean_first_page_text(ad_text[0]).lower()\n",
    "        \n",
    "        if found_group in exclude:\n",
    "            continue\n",
    "        \n",
    "        next_page_tokens = ['like page', 'instagram', 'suggested page', 'sponsored', 'post not exist']\n",
    "        for next_page_token in next_page_tokens:\n",
    "            if next_page_token in found_group or found_group in next_page_token or similar(found_group, next_page_token) > 0.9:\n",
    "                found_group = ad_text[1].lower()\n",
    "                \n",
    "        if found_group in ['post not exist: post that is used for', 'sponsored', 'this preview does not exist.']:\n",
    "            continue\n",
    "                \n",
    "        if found_group == 'mbm' or found_group == 'atch':\n",
    "            found_group = 'bm'\n",
    "        \n",
    "        if found_group == 'atch':\n",
    "            found_group = 'watch the police'\n",
    "        \n",
    "        max_group = {\n",
    "            \"name\": \"\",\n",
    "            \"score\": 0\n",
    "        }\n",
    "        \n",
    "        for group in groups:\n",
    "            if similar(group, found_group) > max_group['score']:\n",
    "                max_group['score'] = similar(group, found_group)\n",
    "                max_group['name'] = group\n",
    "        \n",
    "        found_group = max_group['name']\n",
    "        \n",
    "        if found_group == '':\n",
    "            continue\n",
    "        \n",
    "        results[get_ad_id(ad)] = found_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(results, open('/Users/drewnleonard/Documents/thesis/data/json/group_keys.json', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
